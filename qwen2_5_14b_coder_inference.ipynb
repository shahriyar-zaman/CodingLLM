{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "c6_vRrWkbM7h"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "max_seq_length = 2048  # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True  # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "# Load Qwen2.5 Coder 14B model\n",
        "# Note: You might need to use the quantized version for T4 GPU\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Qwen2.5-Coder-14B-bnb-4bit\",  # Using 4bit quantized version\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models\n",
        ")\n",
        "\n",
        "# Configure LoRA for fine-tuning (if needed)\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,  # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,  # Supports any, but = 0 is optimized\n",
        "    bias=\"none\",  # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long context\n",
        "    random_state=3407,\n",
        "    use_rslora=False,  # We support rank stabilized LoRA\n",
        "    loftq_config=None,  # And LoftQ\n",
        ")\n",
        "\n",
        "# Setup chat template for Qwen2.5\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template=\"qwen-2.5\",\n",
        ")\n",
        "\n",
        "# Enable inference mode\n",
        "FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
        "\n",
        "# Example 1: Basic inference\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Write a Python function to calculate the factorial of a number.\"},\n",
        "]\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=True,\n",
        "    add_generation_prompt=True,  # Must add for generation\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "# Generate response\n",
        "outputs = model.generate(\n",
        "    input_ids=inputs,\n",
        "    max_new_tokens=256,\n",
        "    use_cache=True,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    do_sample=True,\n",
        ")\n",
        "\n",
        "# Decode and print the output\n",
        "response = tokenizer.batch_decode(outputs)\n",
        "print(response[0])\n",
        "\n",
        "# Example 2: Streaming output\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "print(\"Streaming example:\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Explain the concept of recursion in programming with an example.\"},\n",
        "]\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=True,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "# Create text streamer for real-time output\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "\n",
        "# Generate with streaming\n",
        "_ = model.generate(\n",
        "    input_ids=inputs,\n",
        "    streamer=text_streamer,\n",
        "    max_new_tokens=256,\n",
        "    use_cache=True,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    do_sample=True,\n",
        ")\n",
        "\n",
        "# # Example 3: Code generation task\n",
        "# print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "# print(\"Code generation example:\")\n",
        "\n",
        "# messages = [\n",
        "#     {\"role\": \"user\", \"content\": \"Write a Python class for a binary search tree with insert and search methods.\"},\n",
        "# ]\n",
        "\n",
        "# inputs = tokenizer.apply_chat_template(\n",
        "#     messages,\n",
        "#     tokenize=True,\n",
        "#     add_generation_prompt=True,\n",
        "#     return_tensors=\"pt\",\n",
        "# ).to(\"cuda\")\n",
        "\n",
        "# outputs = model.generate(\n",
        "#     input_ids=inputs,\n",
        "#     max_new_tokens=512,\n",
        "#     use_cache=True,\n",
        "#     temperature=0.7,\n",
        "#     top_p=0.9,\n",
        "#     do_sample=True,\n",
        "# )\n",
        "\n",
        "# response = tokenizer.batch_decode(outputs)\n",
        "# print(response[0])\n",
        "\n",
        "# # Optional: Save the model (if you've fine-tuned it)\n",
        "# # model.save_pretrained(\"qwen25-coder-lora\") # Local saving\n",
        "# # tokenizer.save_pretrained(\"qwen25-coder-lora\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3: Code generation task\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "print(\"Code generation example:\")\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Write a Python class for a binary search tree with insert and search methods.\"},\n",
        "]\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=True,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids=inputs,\n",
        "    max_new_tokens=512,\n",
        "    use_cache=True,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    do_sample=True,\n",
        ")\n",
        "\n",
        "response = tokenizer.batch_decode(outputs)\n",
        "print(response[0])\n",
        "\n",
        "# Optional: Save the model (if you've fine-tuned it)\n",
        "# model.save_pretrained(\"qwen25-coder-lora\") # Local saving\n",
        "# tokenizer.save_pretrained(\"qwen25-coder-lora\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_lnPf8PbYXH",
        "outputId": "687466ef-ed8e-4cee-9d83-bad04b019fe1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "\n",
            "Code generation example:\n",
            "<|im_start|>system\n",
            "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "Write a Python class for a binary search tree with insert and search methods.<|im_end|>\n",
            "<|im_start|>assistant\n",
            "Here is a Python implementation of a binary search tree with insert and search methods:\n",
            "\n",
            "```python\n",
            "class TreeNode:\n",
            "    def __init__(self, value):\n",
            "        self.value = value\n",
            "        self.left = None\n",
            "        self.right = None\n",
            "\n",
            "class BinarySearchTree:\n",
            "    def __init__(self):\n",
            "        self.root = None\n",
            "\n",
            "    def insert(self, value):\n",
            "        if self.root is None:\n",
            "            self.root = TreeNode(value)\n",
            "        else:\n",
            "            self._insert(self.root, value)\n",
            "\n",
            "    def _insert(self, node, value):\n",
            "        if value < node.value:\n",
            "            if node.left is None:\n",
            "                node.left = TreeNode(value)\n",
            "            else:\n",
            "                self._insert(node.left, value)\n",
            "        elif value > node.value:\n",
            "            if node.right is None:\n",
            "                node.right = TreeNode(value)\n",
            "            else:\n",
            "                self._insert(node.right, value)\n",
            "\n",
            "    def search(self, value):\n",
            "        return self._search(self.root, value)\n",
            "\n",
            "    def _search(self, node, value):\n",
            "        if node is None or node.value == value:\n",
            "            return node\n",
            "        if value < node.value:\n",
            "            return self._search(node.left, value)\n",
            "        else:\n",
            "            return self._search(node.right, value)\n",
            "```\n",
            "\n",
            "The `TreeNode` class represents a node in the binary search tree, with `value`, `left`, and `right` attributes. The `BinarySearchTree` class has an `insert` method that inserts a new value into the tree and a `search` method that searches for a value in the tree.\n",
            "\n",
            "The `insert` method starts by checking if the tree is empty. If it is, the new value becomes the root node. Otherwise, the `_insert` helper method is called to recursively insert the value into the tree.\n",
            "\n",
            "The `_insert` method first checks if the value is less than the current node's value. If it is, it checks if the current node has a left child. If not, it creates a new left child with the value. If there is a left child, it recursively calls `_insert` on the left child.\n",
            "\n",
            "If the value is greater than the current node's value, the `_insert` method checks if the current node has a right child. If not, it creates a new right child with the value. If there is a right child, it recursively calls `_insert` on the right child.\n",
            "\n",
            "The `search` method starts by calling the `_search` helper method on the root node. The `_search` method recursively\n"
          ]
        }
      ]
    }
  ]
}